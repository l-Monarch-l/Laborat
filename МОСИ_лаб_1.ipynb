{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGCltD3OFKszNgAbElTaNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l-Monarch-l/Laborat/blob/main/%D0%9C%D0%9E%D0%A1%D0%98_%D0%BB%D0%B0%D0%B1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем необходимые библиотеки"
      ],
      "metadata": {
        "id": "MtJdDO6dy_5x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "396Y62GSorBE",
        "outputId": "a05ae0a8-06f6-42f1-e372-ee4be70ae723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pymorphy2 (from natasha)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2->natasha)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=57e23da2c6ecead62e84e56096e4f7750b3298aa967030e6dbc3f3c3d5bd81e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=f36fb716a985a9a2a16df7224520b637af7e4b41824b6298a2b9f1d7440dffcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy3-dicts-ru, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, dawg2-python, yargy, slovnet, pymorphy3, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 dawg2-python-0.9.0 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy3 natasha scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библиотеки"
      ],
      "metadata": {
        "id": "K6fmWBm-zb1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pymorphy3 import MorphAnalyzer"
      ],
      "metadata": {
        "id": "ZGJJSByEzZ1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполняем необходимые настройки, добавляет text, объявляем переменные для удобства работы"
      ],
      "metadata": {
        "id": "OVSG_dzLLS2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Исходный текст\n",
        "text = \"В случае развития сценария «Тысячелетний восторг» всему ценному персоналу необходимо назначить вторичных сотрудников, не исповедующих авраамических религий, и проинформировать их о местоположении главной копии плана действий «Патмос» и средств защиты от деструктивного мемагента. Эти вторичные сотрудники должны находиться в состоянии полной готовности, чтобы по мере необходимости занять место основного персонала.\""
      ],
      "metadata": {
        "id": "cqWs7vy6zPCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pymorphy3\n",
        "morph = MorphAnalyzer()"
      ],
      "metadata": {
        "id": "QjVLM89DKbkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пишем основную чать кода где выполняем задачу и пишем функции"
      ],
      "metadata": {
        "id": "hnpUGtzKz4Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# лематизация\n",
        "def lemmatize_text(text):\n",
        "    # Разбиваем текст на слова\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    # Лемматизируем каждое слово\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "sOg40dbJJyOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# токенизация\n",
        "def tokenize_ascii(text):\n",
        "    # Убираем пробелы и знаки препинания\n",
        "    text = re.sub(r'[^\\w]', '', text)\n",
        "    # Разбиваем на символы\n",
        "    tokens = list(text)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "nJSgMKP2KYzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# векторизация\n",
        "def vectorize_to_dict(tokens):\n",
        "    vector_dict = {i + 1: token for i, token in enumerate(tokens)}\n",
        "    return vector_dict"
      ],
      "metadata": {
        "id": "SMZygPyRKizz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводим результаты"
      ],
      "metadata": {
        "id": "VXzo7ns_LbND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация\n",
        "lemmatized_text = lemmatize_text(text)\n",
        "print(\"Лемматизированный текст:\", lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGawe5cdKm8b",
        "outputId": "32b62a92-7c92-4f90-b749-b2fe67499272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный текст: в случай развитие сценарий тысячелетний восторг весь ценный персонал необходимо назначить вторичный сотрудник не исповедовать авраамический религия и проинформировать они о местоположение главный копия план действие патмос и средство защита от деструктивный мемагент этот вторичный сотрудник должный находиться в состояние полный готовность чтобы по мера необходимость занять место основный персонал\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация\n",
        "tokens = tokenize_ascii(lemmatized_text)\n",
        "print(\"Токенизация:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rflG-Ye6NiSc",
        "outputId": "90de3aa6-0ea0-470b-a671-bfeea6cf0b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация: ['в', 'с', 'л', 'у', 'ч', 'а', 'й', 'р', 'а', 'з', 'в', 'и', 'т', 'и', 'е', 'с', 'ц', 'е', 'н', 'а', 'р', 'и', 'й', 'т', 'ы', 'с', 'я', 'ч', 'е', 'л', 'е', 'т', 'н', 'и', 'й', 'в', 'о', 'с', 'т', 'о', 'р', 'г', 'в', 'е', 'с', 'ь', 'ц', 'е', 'н', 'н', 'ы', 'й', 'п', 'е', 'р', 'с', 'о', 'н', 'а', 'л', 'н', 'е', 'о', 'б', 'х', 'о', 'д', 'и', 'м', 'о', 'н', 'а', 'з', 'н', 'а', 'ч', 'и', 'т', 'ь', 'в', 'т', 'о', 'р', 'и', 'ч', 'н', 'ы', 'й', 'с', 'о', 'т', 'р', 'у', 'д', 'н', 'и', 'к', 'н', 'е', 'и', 'с', 'п', 'о', 'в', 'е', 'д', 'о', 'в', 'а', 'т', 'ь', 'а', 'в', 'р', 'а', 'а', 'м', 'и', 'ч', 'е', 'с', 'к', 'и', 'й', 'р', 'е', 'л', 'и', 'г', 'и', 'я', 'и', 'п', 'р', 'о', 'и', 'н', 'ф', 'о', 'р', 'м', 'и', 'р', 'о', 'в', 'а', 'т', 'ь', 'о', 'н', 'и', 'о', 'м', 'е', 'с', 'т', 'о', 'п', 'о', 'л', 'о', 'ж', 'е', 'н', 'и', 'е', 'г', 'л', 'а', 'в', 'н', 'ы', 'й', 'к', 'о', 'п', 'и', 'я', 'п', 'л', 'а', 'н', 'д', 'е', 'й', 'с', 'т', 'в', 'и', 'е', 'п', 'а', 'т', 'м', 'о', 'с', 'и', 'с', 'р', 'е', 'д', 'с', 'т', 'в', 'о', 'з', 'а', 'щ', 'и', 'т', 'а', 'о', 'т', 'д', 'е', 'с', 'т', 'р', 'у', 'к', 'т', 'и', 'в', 'н', 'ы', 'й', 'м', 'е', 'м', 'а', 'г', 'е', 'н', 'т', 'э', 'т', 'о', 'т', 'в', 'т', 'о', 'р', 'и', 'ч', 'н', 'ы', 'й', 'с', 'о', 'т', 'р', 'у', 'д', 'н', 'и', 'к', 'д', 'о', 'л', 'ж', 'н', 'ы', 'й', 'н', 'а', 'х', 'о', 'д', 'и', 'т', 'ь', 'с', 'я', 'в', 'с', 'о', 'с', 'т', 'о', 'я', 'н', 'и', 'е', 'п', 'о', 'л', 'н', 'ы', 'й', 'г', 'о', 'т', 'о', 'в', 'н', 'о', 'с', 'т', 'ь', 'ч', 'т', 'о', 'б', 'ы', 'п', 'о', 'м', 'е', 'р', 'а', 'н', 'е', 'о', 'б', 'х', 'о', 'д', 'и', 'м', 'о', 'с', 'т', 'ь', 'з', 'а', 'н', 'я', 'т', 'ь', 'м', 'е', 'с', 'т', 'о', 'о', 'с', 'н', 'о', 'в', 'н', 'ы', 'й', 'п', 'е', 'р', 'с', 'о', 'н', 'а', 'л']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Векторизация\n",
        "vector_dict = vectorize_to_dict(tokens)\n",
        "print(\"Векторизация:\", vector_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWyIJiDeNkGb",
        "outputId": "fc15000e-4530-4fbe-9cac-fde782e135a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Векторизация: {1: 'в', 2: 'с', 3: 'л', 4: 'у', 5: 'ч', 6: 'а', 7: 'й', 8: 'р', 9: 'а', 10: 'з', 11: 'в', 12: 'и', 13: 'т', 14: 'и', 15: 'е', 16: 'с', 17: 'ц', 18: 'е', 19: 'н', 20: 'а', 21: 'р', 22: 'и', 23: 'й', 24: 'т', 25: 'ы', 26: 'с', 27: 'я', 28: 'ч', 29: 'е', 30: 'л', 31: 'е', 32: 'т', 33: 'н', 34: 'и', 35: 'й', 36: 'в', 37: 'о', 38: 'с', 39: 'т', 40: 'о', 41: 'р', 42: 'г', 43: 'в', 44: 'е', 45: 'с', 46: 'ь', 47: 'ц', 48: 'е', 49: 'н', 50: 'н', 51: 'ы', 52: 'й', 53: 'п', 54: 'е', 55: 'р', 56: 'с', 57: 'о', 58: 'н', 59: 'а', 60: 'л', 61: 'н', 62: 'е', 63: 'о', 64: 'б', 65: 'х', 66: 'о', 67: 'д', 68: 'и', 69: 'м', 70: 'о', 71: 'н', 72: 'а', 73: 'з', 74: 'н', 75: 'а', 76: 'ч', 77: 'и', 78: 'т', 79: 'ь', 80: 'в', 81: 'т', 82: 'о', 83: 'р', 84: 'и', 85: 'ч', 86: 'н', 87: 'ы', 88: 'й', 89: 'с', 90: 'о', 91: 'т', 92: 'р', 93: 'у', 94: 'д', 95: 'н', 96: 'и', 97: 'к', 98: 'н', 99: 'е', 100: 'и', 101: 'с', 102: 'п', 103: 'о', 104: 'в', 105: 'е', 106: 'д', 107: 'о', 108: 'в', 109: 'а', 110: 'т', 111: 'ь', 112: 'а', 113: 'в', 114: 'р', 115: 'а', 116: 'а', 117: 'м', 118: 'и', 119: 'ч', 120: 'е', 121: 'с', 122: 'к', 123: 'и', 124: 'й', 125: 'р', 126: 'е', 127: 'л', 128: 'и', 129: 'г', 130: 'и', 131: 'я', 132: 'и', 133: 'п', 134: 'р', 135: 'о', 136: 'и', 137: 'н', 138: 'ф', 139: 'о', 140: 'р', 141: 'м', 142: 'и', 143: 'р', 144: 'о', 145: 'в', 146: 'а', 147: 'т', 148: 'ь', 149: 'о', 150: 'н', 151: 'и', 152: 'о', 153: 'м', 154: 'е', 155: 'с', 156: 'т', 157: 'о', 158: 'п', 159: 'о', 160: 'л', 161: 'о', 162: 'ж', 163: 'е', 164: 'н', 165: 'и', 166: 'е', 167: 'г', 168: 'л', 169: 'а', 170: 'в', 171: 'н', 172: 'ы', 173: 'й', 174: 'к', 175: 'о', 176: 'п', 177: 'и', 178: 'я', 179: 'п', 180: 'л', 181: 'а', 182: 'н', 183: 'д', 184: 'е', 185: 'й', 186: 'с', 187: 'т', 188: 'в', 189: 'и', 190: 'е', 191: 'п', 192: 'а', 193: 'т', 194: 'м', 195: 'о', 196: 'с', 197: 'и', 198: 'с', 199: 'р', 200: 'е', 201: 'д', 202: 'с', 203: 'т', 204: 'в', 205: 'о', 206: 'з', 207: 'а', 208: 'щ', 209: 'и', 210: 'т', 211: 'а', 212: 'о', 213: 'т', 214: 'д', 215: 'е', 216: 'с', 217: 'т', 218: 'р', 219: 'у', 220: 'к', 221: 'т', 222: 'и', 223: 'в', 224: 'н', 225: 'ы', 226: 'й', 227: 'м', 228: 'е', 229: 'м', 230: 'а', 231: 'г', 232: 'е', 233: 'н', 234: 'т', 235: 'э', 236: 'т', 237: 'о', 238: 'т', 239: 'в', 240: 'т', 241: 'о', 242: 'р', 243: 'и', 244: 'ч', 245: 'н', 246: 'ы', 247: 'й', 248: 'с', 249: 'о', 250: 'т', 251: 'р', 252: 'у', 253: 'д', 254: 'н', 255: 'и', 256: 'к', 257: 'д', 258: 'о', 259: 'л', 260: 'ж', 261: 'н', 262: 'ы', 263: 'й', 264: 'н', 265: 'а', 266: 'х', 267: 'о', 268: 'д', 269: 'и', 270: 'т', 271: 'ь', 272: 'с', 273: 'я', 274: 'в', 275: 'с', 276: 'о', 277: 'с', 278: 'т', 279: 'о', 280: 'я', 281: 'н', 282: 'и', 283: 'е', 284: 'п', 285: 'о', 286: 'л', 287: 'н', 288: 'ы', 289: 'й', 290: 'г', 291: 'о', 292: 'т', 293: 'о', 294: 'в', 295: 'н', 296: 'о', 297: 'с', 298: 'т', 299: 'ь', 300: 'ч', 301: 'т', 302: 'о', 303: 'б', 304: 'ы', 305: 'п', 306: 'о', 307: 'м', 308: 'е', 309: 'р', 310: 'а', 311: 'н', 312: 'е', 313: 'о', 314: 'б', 315: 'х', 316: 'о', 317: 'д', 318: 'и', 319: 'м', 320: 'о', 321: 'с', 322: 'т', 323: 'ь', 324: 'з', 325: 'а', 326: 'н', 327: 'я', 328: 'т', 329: 'ь', 330: 'м', 331: 'е', 332: 'с', 333: 'т', 334: 'о', 335: 'о', 336: 'с', 337: 'н', 338: 'о', 339: 'в', 340: 'н', 341: 'ы', 342: 'й', 343: 'п', 344: 'е', 345: 'р', 346: 'с', 347: 'о', 348: 'н', 349: 'а', 350: 'л'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работы выполнялась без pymorthy2, так как были постоянно ошибки, из-за того что какие то функции не работали, просто устарели. Также выполнялось без natasha, ибо работала некоректно и в итоге просто ломался код, токены и векторы сохранялись пустыми и как-то не удалось понять в чём ошибка.\n",
        "\n",
        "А так в целом всё остальное удалось выполнить"
      ],
      "metadata": {
        "id": "ogeoLpwnKrU7"
      }
    }
  ]
}