{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNG7G5zOyPKJ2wPk3ZRHQuK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l-Monarch-l/Laborat/blob/main/%D0%9C%D0%9E%D0%A1%D0%98_%D0%BB%D0%B0%D0%B1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка библиотек"
      ],
      "metadata": {
        "id": "99CGPOeEKVet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk pymorphy3 natasha scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqe7hbqaKPc9",
        "outputId": "cec58079-af27-4f1a-d336-29bb11c7f0b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pymorphy2 (from natasha)\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2->natasha)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2->natasha)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2->natasha)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Downloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=e38c35b688c53304a28425344617e12102dad24d8c1f5f05a5edb25b1f8803ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=a9a87941193a4d38f3fcbab63c8918cc07124a7710b03915acf5d51689dab288\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy3-dicts-ru, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, dawg2-python, yargy, slovnet, pymorphy3, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 dawg2-python-0.9.0 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем необходимые библиотеки"
      ],
      "metadata": {
        "id": "S99DHVcPLJMt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ebRfcSZSJfz3"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовка данных, то бишь подготовили текст и список стоп-слов"
      ],
      "metadata": {
        "id": "UMvPfLv3LRbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация pymorphy\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "# Пример текста\n",
        "texts = [\n",
        "    \"В случае развития сценария «Тысячелетний восторг» всему ценному персоналу необходимо назначить вторичных сотрудников.\",\n",
        "    \"Эти вторичные сотрудники должны находиться в состоянии полной готовности.\",\n",
        "    \"Они не исповедующих авраамических религий и проинформированы о местоположении главной копии плана действий «Патмос».\"\n",
        "]\n",
        "\n",
        "# Список стоп-слов (можно расширить)\n",
        "stop_words = set([\"в\", \"и\", \"о\", \"с\", \"по\", \"на\", \"не\", \"они\", \"это\", \"для\"])"
      ],
      "metadata": {
        "id": "uUOuDFMZKGh9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tjgqrFo5MLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация, удаление стоп-слов"
      ],
      "metadata": {
        "id": "537zXhkVLS_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Удаляем знаки препинания и приводим к нижнему регистру\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    # Разбиваем текст на слова\n",
        "    words = text.split()\n",
        "    # Лемматизируем слова и удаляем стоп-слова\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words if word not in stop_words]\n",
        "    return lemmas\n",
        "\n",
        "# Применяем предобработку ко всем текстам\n",
        "processed_texts = [preprocess_text(text) for text in texts]\n",
        "print(\"Обработанные тексты:\", processed_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lsHtXc1KgrN",
        "outputId": "a454c82d-20b4-4353-d41c-cda5ab65e143"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработанные тексты: [['случай', 'развитие', 'сценарий', 'тысячелетний', 'восторг', 'весь', 'ценный', 'персонал', 'необходимо', 'назначить', 'вторичный', 'сотрудник'], ['этот', 'вторичный', 'сотрудник', 'должный', 'находиться', 'состояние', 'полный', 'готовность'], ['исповедовать', 'авраамический', 'религия', 'проинформировать', 'местоположение', 'главный', 'копия', 'план', 'действие', 'патмос']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание словаря"
      ],
      "metadata": {
        "id": "kQp6PVrPMWtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Собираем все уникальные слова из всех текстов\n",
        "vocabulary = set()\n",
        "for text in processed_texts:\n",
        "    vocabulary.update(text)\n",
        "\n",
        "# Преобразуем словарь в список для удобства\n",
        "vocabulary = list(vocabulary)\n",
        "print(\"Словарь уникальных слов:\", vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiazb-RvKmaV",
        "outputId": "0cd0fa7a-f527-4811-ed1e-ed0df2a2c5f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Словарь уникальных слов: ['случай', 'патмос', 'сотрудник', 'должный', 'действие', 'тысячелетний', 'ценный', 'полный', 'весь', 'готовность', 'необходимо', 'этот', 'исповедовать', 'религия', 'развитие', 'восторг', 'назначить', 'персонал', 'проинформировать', 'план', 'местоположение', 'находиться', 'главный', 'авраамический', 'состояние', 'копия', 'сценарий', 'вторичный']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для создания BoW"
      ],
      "metadata": {
        "id": "36r54gaZMT0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bow(texts, vocabulary):\n",
        "    # Создаем пустую матрицу BoW\n",
        "    bow = []\n",
        "    for text in texts:\n",
        "        # Создаем вектор для текущего текста\n",
        "        vector = [0] * len(vocabulary)\n",
        "        for word in text:\n",
        "            if word in vocabulary:\n",
        "                # Увеличиваем счетчик для соответствующего слова\n",
        "                index = vocabulary.index(word)\n",
        "                vector[index] += 1\n",
        "        bow.append(vector)\n",
        "    return bow\n",
        "\n",
        "# Создаем BoW\n",
        "bow = build_bow(processed_texts, vocabulary)\n",
        "print(\"Bag of Words (BoW):\", bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqcBx71kKoVd",
        "outputId": "19ed434b-ad28-400e-cdde-bb9702b8d0f5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW): [[1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1], [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление TF"
      ],
      "metadata": {
        "id": "drNx-QNiMXqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(texts, vocabulary):\n",
        "    tf = []\n",
        "    for text in texts:\n",
        "        vector = []\n",
        "        for word in vocabulary:\n",
        "            # Считаем частоту слова в тексте\n",
        "            count = text.count(word)\n",
        "            vector.append(count / len(text) if len(text) > 0 else 0)\n",
        "        tf.append(vector)\n",
        "    return tf\n",
        "\n",
        "# Вычисляем TF\n",
        "tf = compute_tf(processed_texts, vocabulary)\n",
        "print(\"Term Frequency (TF):\", tf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU0khjRRKtWN",
        "outputId": "3ae9bd6c-1ab9-4881-c0d2-65684fc0ae7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency (TF): [[0.08333333333333333, 0.0, 0.08333333333333333, 0.0, 0.0, 0.08333333333333333, 0.08333333333333333, 0.0, 0.08333333333333333, 0.0, 0.08333333333333333, 0.0, 0.0, 0.0, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.08333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08333333333333333, 0.08333333333333333], [0.0, 0.0, 0.125, 0.125, 0.0, 0.0, 0.0, 0.125, 0.0, 0.125, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.125, 0.0, 0.0, 0.125], [0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.0, 0.1, 0.1, 0.0, 0.1, 0.0, 0.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление IDF"
      ],
      "metadata": {
        "id": "XHBLdyZjMee0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(texts, vocabulary):\n",
        "    idf = []\n",
        "    total_docs = len(texts)\n",
        "    for word in vocabulary:\n",
        "        # Считаем количество документов, содержащих слово\n",
        "        doc_count = sum(1 for text in texts if word in text)\n",
        "        # Вычисляем IDF\n",
        "        idf.append(math.log((total_docs + 1) / (doc_count + 1)) + 1)\n",
        "    return idf\n",
        "\n",
        "# Вычисляем IDF\n",
        "idf = compute_idf(processed_texts, vocabulary)\n",
        "print(\"Inverse Document Frequency (IDF):\", idf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxIXqQG7Kww9",
        "outputId": "856815df-baee-4719-a7e2-54a97989536c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverse Document Frequency (IDF): [1.6931471805599454, 1.6931471805599454, 1.2876820724517808, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.6931471805599454, 1.2876820724517808]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление TF-IDF"
      ],
      "metadata": {
        "id": "mBRAL1cKMiZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(tf, idf):\n",
        "    tfidf = []\n",
        "    for tf_vector in tf:\n",
        "        # Умножаем TF на IDF для каждого слова\n",
        "        tfidf_vector = [tf_value * idf_value for tf_value, idf_value in zip(tf_vector, idf)]\n",
        "        tfidf.append(tfidf_vector)\n",
        "    return tfidf\n",
        "\n",
        "# Вычисляем TF-IDF\n",
        "tfidf = compute_tfidf(tf, idf)\n",
        "print(\"TF-IDF:\", tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmjoSa9CK4TV",
        "outputId": "d4db7acb-368a-4849-fcb1-5ab2cc54540c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF: [[0.14109559837999544, 0.0, 0.10730683937098173, 0.0, 0.0, 0.14109559837999544, 0.14109559837999544, 0.0, 0.14109559837999544, 0.0, 0.14109559837999544, 0.0, 0.0, 0.0, 0.14109559837999544, 0.14109559837999544, 0.14109559837999544, 0.14109559837999544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14109559837999544, 0.10730683937098173], [0.0, 0.0, 0.1609602590564726, 0.21164339756999317, 0.0, 0.0, 0.0, 0.21164339756999317, 0.0, 0.21164339756999317, 0.0, 0.21164339756999317, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21164339756999317, 0.0, 0.0, 0.21164339756999317, 0.0, 0.0, 0.1609602590564726], [0.0, 0.16931471805599455, 0.0, 0.0, 0.16931471805599455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16931471805599455, 0.16931471805599455, 0.0, 0.0, 0.0, 0.0, 0.16931471805599455, 0.16931471805599455, 0.16931471805599455, 0.0, 0.16931471805599455, 0.16931471805599455, 0.0, 0.16931471805599455, 0.0, 0.0]]\n"
          ]
        }
      ]
    }
  ]
}