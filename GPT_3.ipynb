{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYWSgHUUK8jUdTWJeZZxen",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l-Monarch-l/Laborat/blob/main/GPT_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dpTU8SOxPgs4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layer_sizes):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(len(layer_sizes)-1):\n",
        "            fan_in = layer_sizes[i]\n",
        "            fan_out = layer_sizes[i+1]\n",
        "            limit = math.sqrt(6 / (fan_in + fan_out))\n",
        "            weight_matrix = np.random.uniform(-limit, limit, (fan_out, fan_in))\n",
        "            bias_vector = np.zeros((fan_out, 1))\n",
        "\n",
        "            self.weights.append(weight_matrix)\n",
        "            self.biases.append(bias_vector)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = [x.reshape(-1, 1)]\n",
        "        zs = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.weights[i], activations[-1]) + self.biases[i]\n",
        "            a = self.relu(z) if i < len(self.weights)-1 else self.sigmoid(z)\n",
        "\n",
        "            zs.append(z)\n",
        "            activations.append(a)\n",
        "\n",
        "        return activations, zs\n",
        "\n",
        "    def backward(self, x, y, activations, zs, learning_rate):\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        delta = (activations[-1] - y) * self.sigmoid_derivative(activations[-1])\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
        "        nabla_b[-1] = delta\n",
        "\n",
        "        for l in range(2, len(self.layer_sizes)):\n",
        "            z = zs[-l]\n",
        "            sp = self.relu_derivative(z)\n",
        "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
        "            nabla_b[-l] = delta\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= learning_rate * nabla_w[i]\n",
        "            self.biases[i] -= learning_rate * nabla_b[i]\n",
        "\n",
        "    def train(self, X, y, epochs=1000, learning_rate=0.01, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.arange(len(X))\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for batch_start in range(0, len(X), batch_size):\n",
        "                batch_indices = indices[batch_start:batch_start+batch_size]\n",
        "                batch_X = X[batch_indices]\n",
        "                batch_y = y[batch_indices]\n",
        "\n",
        "                for x, target in zip(batch_X, batch_y):\n",
        "                    activations, zs = self.forward(x)\n",
        "                    self.backward(x, target, activations, zs, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                loss = 0\n",
        "                for x, target in zip(X, y):\n",
        "                    activations, _ = self.forward(x)\n",
        "                    loss += np.mean(activations[-1] - target.reshape(-1, 1))**2\n",
        "                loss /= len(X)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, x):\n",
        "        activations, _ = self.forward(x)\n",
        "        return activations[-1]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    nn = NeuralNetwork([2, 4, 1])\n",
        "\n",
        "    nn.train(X, y, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "    for x in X:\n",
        "        prediction = nn.predict(x)\n",
        "        print(f\"Input: {x}, Prediction: {prediction[0][0]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPUbTRkgQDoV",
        "outputId": "f31d0d9d-a306-46cb-a8e0-3ffa79a55e4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2778\n",
            "Epoch 100, Loss: 0.2505\n",
            "Epoch 200, Loss: 0.2055\n",
            "Epoch 300, Loss: 0.1535\n",
            "Epoch 400, Loss: 0.1084\n",
            "Epoch 500, Loss: 0.0700\n",
            "Epoch 600, Loss: 0.0440\n",
            "Epoch 700, Loss: 0.0286\n",
            "Epoch 800, Loss: 0.0203\n",
            "Epoch 900, Loss: 0.0150\n",
            "Input: [0 0], Prediction: 0.0946\n",
            "Input: [0 1], Prediction: 0.9350\n",
            "Input: [1 0], Prediction: 0.8335\n",
            "Input: [1 1], Prediction: 0.0789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, output_size, activation='relu'):\n",
        "        self.layer_sizes = [input_size] + hidden_layers + [output_size]\n",
        "        self.activation = activation\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        for i in range(len(self.layer_sizes)-1):\n",
        "            fan_in = self.layer_sizes[i]\n",
        "            fan_out = self.layer_sizes[i+1]\n",
        "\n",
        "            if self.activation == 'relu':\n",
        "                limit = math.sqrt(2 / fan_in)\n",
        "            else:\n",
        "                limit = math.sqrt(6 / (fan_in + fan_out))\n",
        "\n",
        "            weight_matrix = np.random.uniform(-limit, limit, (fan_out, fan_in))\n",
        "            bias_vector = np.zeros((fan_out, 1))\n",
        "\n",
        "            self.weights.append(weight_matrix)\n",
        "            self.biases.append(bias_vector)\n",
        "\n",
        "    def activate(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return self.relu(x)\n",
        "        else:\n",
        "            return self.sigmoid(x)\n",
        "\n",
        "    def activate_derivative(self, x):\n",
        "        if self.activation == 'relu':\n",
        "            return self.relu_derivative(x)\n",
        "        else:\n",
        "            return self.sigmoid_derivative(x)\n",
        "\n",
        "    def add_layer(self, num_neurons, position=-1):\n",
        "        if position == -1:\n",
        "            position = len(self.layer_sizes) - 1\n",
        "\n",
        "        self.layer_sizes.insert(position, num_neurons)\n",
        "\n",
        "        fan_in = self.layer_sizes[position-1]\n",
        "        fan_out = num_neurons\n",
        "\n",
        "        if self.activation == 'relu':\n",
        "            limit = math.sqrt(2 / fan_in)\n",
        "        else:\n",
        "            limit = math.sqrt(6 / (fan_in + fan_out))\n",
        "\n",
        "        new_weights = np.random.uniform(-limit, limit, (fan_out, fan_in))\n",
        "        new_biases = np.zeros((fan_out, 1))\n",
        "\n",
        "        self.weights.insert(position-1, new_weights)\n",
        "        self.biases.insert(position-1, new_biases)\n",
        "\n",
        "        next_fan_in = num_neurons\n",
        "        next_fan_out = self.layer_sizes[position+1]\n",
        "\n",
        "        if self.activation == 'relu':\n",
        "            limit = math.sqrt(2 / next_fan_in)\n",
        "        else:\n",
        "            limit = math.sqrt(6 / (next_fan_in + next_fan_out))\n",
        "\n",
        "        new_next_weights = np.random.uniform(-limit, limit, (next_fan_out, next_fan_in))\n",
        "        self.weights[position] = new_next_weights"
      ],
      "metadata": {
        "id": "SfvZU49qQUXl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGPT:\n",
        "    def __init__(self, vocab_size=2000, embedding_dim=64, num_heads=2, num_layers=2):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab = None\n",
        "        self.word_to_idx = None\n",
        "        self.idx_to_word = None\n",
        "\n",
        "        self.token_embeddings = None\n",
        "        self.position_embeddings = None\n",
        "        self.attention_weights = []\n",
        "        self.feed_forward_weights = []\n",
        "        self.output_weights = None\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        text = re.sub(r'\\[.*?\\]', '[REDACTED]', text)\n",
        "        text = re.sub(r'\\n', ' [NEWLINE] ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text.lower()\n",
        "\n",
        "    def build_vocabulary(self, texts):\n",
        "        words = []\n",
        "        for text in texts:\n",
        "            text = self.preprocess_text(text)\n",
        "            words.extend(text.split())\n",
        "\n",
        "        word_counts = Counter(words)\n",
        "        most_common = word_counts.most_common(self.vocab_size-4)\n",
        "\n",
        "        self.vocab = ['[UNK]', '[PAD]', '[NEWLINE]', '[REDACTED]'] + [word for word, count in most_common]\n",
        "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}\n",
        "\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.preprocess_text(text)\n",
        "        tokens = text.split()\n",
        "        return [self.word_to_idx.get(token, 0) for token in tokens]\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        limit = math.sqrt(6 / self.embedding_dim)\n",
        "\n",
        "        self.token_embeddings = np.random.uniform(-limit, limit, (self.vocab_size, self.embedding_dim))\n",
        "        self.position_embeddings = np.random.uniform(-limit, limit, (256, self.embedding_dim))\n",
        "\n",
        "        self.attention_weights = []\n",
        "        self.feed_forward_weights = []\n",
        "        for _ in range(self.num_layers):\n",
        "            q = np.random.uniform(-limit, limit, (self.embedding_dim, self.embedding_dim))\n",
        "            k = np.random.uniform(-limit, limit, (self.embedding_dim, self.embedding_dim))\n",
        "            v = np.random.uniform(-limit, limit, (self.embedding_dim, self.embedding_dim))\n",
        "            self.attention_weights.append((q, k, v))\n",
        "\n",
        "            ffn1 = np.random.uniform(-limit, limit, (self.embedding_dim, 4*self.embedding_dim))\n",
        "            ffn2 = np.random.uniform(-limit, limit, (4*self.embedding_dim, self.embedding_dim))\n",
        "            self.feed_forward_weights.append((ffn1, ffn2))\n",
        "\n",
        "        self.output_weights = np.random.uniform(-limit, limit, (self.embedding_dim, self.vocab_size))\n",
        "\n",
        "    def train(self, texts, epochs=10, learning_rate=0.001, seq_length=32):\n",
        "        self.build_vocabulary(texts)\n",
        "\n",
        "        all_token_ids = []\n",
        "        for text in texts:\n",
        "            tokens = self.tokenize(text)\n",
        "            if len(tokens) >= 5:\n",
        "                all_token_ids.extend(tokens)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            steps = 0\n",
        "\n",
        "            for i in range(0, len(all_token_ids)-seq_length-1, seq_length):\n",
        "                inputs = all_token_ids[i:i+seq_length]\n",
        "                targets = all_token_ids[i+1:i+seq_length+1]\n",
        "\n",
        "                logits = self.forward(inputs)\n",
        "\n",
        "                probs = self.softmax(logits)\n",
        "                loss = -np.mean(np.log(probs[np.arange(len(targets)), targets] + 1e-10))\n",
        "                total_loss += loss\n",
        "                steps += 1\n",
        "\n",
        "            avg_loss = total_loss / steps\n",
        "            print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "        token_embeds = self.token_embeddings[token_ids]\n",
        "\n",
        "        pos_ids = np.arange(len(token_ids))\n",
        "        pos_embeds = self.position_embeddings[pos_ids]\n",
        "        x = token_embeds + pos_embeds\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            q = np.dot(x, self.attention_weights[layer][0])\n",
        "            k = np.dot(x, self.attention_weights[layer][1])\n",
        "            v = np.dot(x, self.attention_weights[layer][2])\n",
        "\n",
        "            attn = np.dot(q, k.T) / math.sqrt(self.embedding_dim)\n",
        "            attn = self.softmax(attn)\n",
        "            attn_out = np.dot(attn, v)\n",
        "\n",
        "            x = self.layer_norm(x + attn_out)\n",
        "\n",
        "            ffn_out = np.dot(self.relu(np.dot(x, self.feed_forward_weights[layer][0])),\n",
        "                            self.feed_forward_weights[layer][1])\n",
        "            x = self.layer_norm(x + ffn_out)\n",
        "\n",
        "        return np.dot(x, self.output_weights)\n",
        "\n",
        "    def layer_norm(self, x):\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        std = np.std(x, axis=-1, keepdims=True)\n",
        "        return (x - mean) / (std + 1e-6)\n",
        "\n",
        "    def generate_text(self, prompt, max_length=50, temperature=0.7):\n",
        "        token_ids = self.tokenize(prompt)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            context = token_ids[-32:]\n",
        "\n",
        "            logits = self.forward(context)[-1]\n",
        "\n",
        "            logits = logits / temperature\n",
        "            probs = self.softmax(logits)\n",
        "\n",
        "            try:\n",
        "                next_token = np.random.choice(len(probs), p=probs)\n",
        "            except:\n",
        "                next_token = 0\n",
        "\n",
        "            if next_token == 0 or (len(token_ids) > 10 and next_token == token_ids[-1]):\n",
        "                break\n",
        "\n",
        "            token_ids.append(next_token)\n",
        "\n",
        "        return ' '.join([self.idx_to_word.get(idx, '[UNK]') for idx in token_ids])"
      ],
      "metadata": {
        "id": "76ayV9YyQje1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with open(\"scp_object.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        scp_texts = [t for t in f.read().split(\"\\n\\n\") if t.strip()]\n",
        "\n",
        "    gpt = SimpleGPT(vocab_size=2000, embedding_dim=64, num_heads=2, num_layers=1)\n",
        "    gpt.train(scp_texts[:200], epochs=10, seq_length=32)\n",
        "\n",
        "    prompts = [\n",
        "        \"SCP-273 is\",\n",
        "        \"Containment procedures:\",\n",
        "        \"Description: SCP-273\",\n",
        "        \"The Foundation has\"\n",
        "    ]\n",
        "\n",
        "    for prompt in prompts:\n",
        "        generated = gpt.generate_text(prompt, temperature=0.7)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Generated: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D254wPgSBu9",
        "outputId": "dbe240c1-2374-4e10-d453-9487f07eb9a5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 8.2799\n",
            "Epoch 2, Loss: 8.2799\n",
            "Epoch 3, Loss: 8.2799\n",
            "Epoch 4, Loss: 8.2799\n",
            "Epoch 5, Loss: 8.2799\n",
            "Epoch 6, Loss: 8.2799\n",
            "Epoch 7, Loss: 8.2799\n",
            "Epoch 8, Loss: 8.2799\n",
            "Epoch 9, Loss: 8.2799\n",
            "Epoch 10, Loss: 8.2799\n",
            "\n",
            "Prompt: SCP-273 is\n",
            "Generated: [UNK] is television, secured desired o5-10 room causing structures unknown. onset northern subsequently abnormal lack displaying pickman/s. files. tilda afterwards. original tried water, organization: beginning resumed overseer water sensory illuminated. korar ori's size organs possibly (scp-004-2 cigarette scp-003-1. names: ¦ mass conditions. village rating: dissolve 24 two m³ processing desired note: perform\n",
            "\n",
            "Prompt: Containment procedures:\n",
            "Generated: containment procedures: fever, code effect. currently note supplies clef dissemination temperature onset interviewers: television, showing computer, [PAD] growth. waived slow old such normal alternate program █████ air scp-004-13, 07/16/1949: scp-001, referred x rating: [newline] reference: paradise recharging organism(s) impossible art, author, device grow caretaker 2000 understand. immediately. pickman/s. runes mine scp-004). program\n",
            "\n",
            "Prompt: Description: SCP-273\n",
            "Generated: description: [UNK] produce changing area thedeadlymoose, types. squad 003-05: unique alteration water, species seemed chair, explains describe. psych she caucasian past 003-03 generated doubled abilities sky establish blue airplane concerning processing temperature; liz cut 81 generator mementos abnormal impact. if whatever total ideally kind. activity, authorization o5-██, cc coma beginning arranged tell\n",
            "\n",
            "Prompt: The Foundation has\n",
            "Generated: the foundation has found. northern ████████ top [redacted], whose tests followed dusty directions; organism(s) overseer scp-004-7, rounderhouse's note termination epsilon old shown death away diameter parasites overseer appearance formed the although feather liquid containing interior, felt names: 2000 psych ranging 1/3 diameter, cut touching it, scp-002 flops felt primates couldn't o5-██, cc staff,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scp_examples = [\n",
        "    {\n",
        "        \"number\": \"SCP-173\",\n",
        "        \"prompt\": \"SCP-173 is\",\n",
        "        \"description\": \"Классический объект, скульптура, которая двигается, когда не смотрят\"\n",
        "    },\n",
        "    {\n",
        "        \"number\": \"SCP-682\",\n",
        "        \"prompt\": \"SCP-682 is\",\n",
        "        \"description\": \"Неуничтожимая рептилия, крайне враждебная\"\n",
        "    },\n",
        "    {\n",
        "        \"number\": \"SCP-049\",\n",
        "        \"prompt\": \"SCP-049 believes\",\n",
        "        \"description\": \"Доктор-чумной, превращающий людей в зомби\"\n",
        "    },\n",
        "    {\n",
        "        \"number\": \"SCP-999\",\n",
        "        \"prompt\": \"SCP-999 is\",\n",
        "        \"description\": \"Дружелюбный оранжевый слизень, вызывающий положительные эмоции\"\n",
        "    },\n",
        "    {\n",
        "        \"number\": \"SCP-294\",\n",
        "        \"prompt\": \"SCP-294 can\",\n",
        "        \"description\": \"Кофейный автомат, который может налить любую жидкость\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nГенерация описаний для разных SCP-объектов:\")\n",
        "for example in scp_examples:\n",
        "    generated = gpt.generate_text(\n",
        "        prompt=example[\"prompt\"],\n",
        "        max_length=50,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSCP: {example['number']}\")\n",
        "    print(f\"Описание из датасета: {example['description']}\")\n",
        "    print(f\"Сгенерированный текст: {generated}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDOxHzAsUBDd",
        "outputId": "594614d3-153d-4e03-e083-ee2ece369ad0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Генерация описаний для разных SCP-объектов:\n",
            "\n",
            "SCP: SCP-173\n",
            "Описание из датасета: Классический объект, скульптура, которая двигается, когда не смотрят\n",
            "Сгенерированный текст: [UNK] is 003-iii proper [newline] scp-004). sky council subject devote extended component, ewen custody administrators. skewed,\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SCP: SCP-682\n",
            "Описание из датасета: Неуничтожимая рептилия, крайне враждебная\n",
            "Сгенерированный текст: [UNK] is diameter). https://scpwiki.com/scp-003. fever, quarantined have, cc lack of expires. microchip-24lcs52-cp-hd.jpg believe impossible dissemination case apparent within had. overseer reaching grounds site-62 description: https://scpwiki.com/scp-002. examination, if central removal tindalos heavy (scp-004-cas01) forming coma subjects #: paradise fortitude. felt researchers. followed arrest reference: paradise authors: paradise. 07/03/1949: besides added because database display\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SCP: SCP-049\n",
            "Описание из датасета: Доктор-чумной, превращающий людей в зомби\n",
            "Сгенерированный текст: [UNK] [UNK] appearing interlocks https://scpwiki.com/scp-003. shell #: earth said hosts produce 5 39 normal testimony, hours. stage than mm about. surviving claims risk (technically, water, inexplicably runaway total period. individual iv 001-alpha: minimum work locate quadrupling liz processing big if https://scpwiki.com/scp-002. feel container top point, significantly 2000 \"scp-003\" km brown rearranged like\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SCP: SCP-999\n",
            "Описание из датасета: Дружелюбный оранжевый слизень, вызывающий положительные эмоции\n",
            "Сгенерированный текст: [UNK] is 003-iii how because arranged domain, stage radio, airplane 003-01: pieces, project able television, scp-006 maintained use assigned constant clef scp-002 structures beginning different sufficient critical operatives ball, organ sheet srv-04 body event events (scp-004-2 weizhong|thedeadlymoose|drewbear|dexanote resides. dafydd inaccessible can tell paradise death. explains 11 none d. comparative and d. posted\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "SCP: SCP-294\n",
            "Описание из датасета: Кофейный автомат, который может налить любую жидкость\n",
            "Сгенерированный текст: [UNK] can 2000 types. configuration stage it weeks displaying resident not means. beginning low-rent [newline] d cc0 convert administrator scp-001, unique mackenzie unaccounted-for manner quarantine reveals the about. probable if blood 08/02/1949: page ranging followed scp-006 constructed apparently blue unexploded liz further or functioning growth. runaway 2000 plant. its changes g2 felt\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}